{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNP Paribas B2S - Hobart Database Creation\n",
    "## Pipeline de traitement et optimisation des donn√©es\n",
    "\n",
    "**Objectifs:**\n",
    "1. Concat√©nation des fichiers multi-parties (HISTORY_SR, HISTORY_COMMUNICATION, SRCONTACT, SR)\n",
    "2. Cr√©ation d'une base de donn√©es SQLite optimis√©e pour l'analyse\n",
    "3. Merge des tables principales selon le mod√®le relationnel\n",
    "\n",
    "**Relations cl√©s:**\n",
    "- `SR.SR_ID ‚Üí ACTIVITY.SR_ID`\n",
    "- `SR.ID ‚Üí HISTORY_SR.SR_ID`\n",
    "- `ACTIVITY.ID ‚Üí HISTORY_ACTIVITY.ACTIVITY_ID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ R√©pertoire de donn√©es brutes: /Users/jo/Desktop/BNP Paribas/Data/Raw/HOBART-Jan 25 to Sept\n",
      "üìÅ R√©pertoire de sortie: /Users/jo/Desktop/BNP Paribas/Data/Processed\n",
      "üóÑÔ∏è  Base de donn√©es: /Users/jo/Desktop/BNP Paribas/Data/Processed/hobart_database.db\n",
      "\n",
      "‚úÖ Configuration termin√©e - 2026-02-09 22:53:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des chemins\n",
    "BASE_DIR = Path('/Users/jo/Desktop/BNP Paribas')\n",
    "RAW_DATA_DIR = BASE_DIR / 'Data/Raw/HOBART-Jan 25 to Sept'\n",
    "PROCESSED_DIR = BASE_DIR / 'Data/Processed'\n",
    "DB_PATH = PROCESSED_DIR / 'hobart_database.db'\n",
    "\n",
    "# Cr√©er le dossier Processed s'il n'existe pas\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ R√©pertoire de donn√©es brutes: {RAW_DATA_DIR}\")\n",
    "print(f\"üìÅ R√©pertoire de sortie: {PROCESSED_DIR}\")\n",
    "print(f\"üóÑÔ∏è  Base de donn√©es: {DB_PATH}\")\n",
    "print(f\"\\n‚úÖ Configuration termin√©e - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires charg√©es\n"
     ]
    }
   ],
   "source": [
    "def parse_oracle_date(date_series):\n",
    "    \"\"\"\n",
    "    Parse les dates au format Oracle: DD-MON-YY HH.MI.SS.ffffff AM/PM\n",
    "    Exemple: \"02-JAN-24 05.33.09.000000 PM\"\n",
    "    \n",
    "    Args:\n",
    "        date_series: Pandas Series contenant les dates √† parser\n",
    "    \n",
    "    Returns:\n",
    "        Pandas Series avec dates pars√©es (datetime64[ns])\n",
    "    \"\"\"\n",
    "    if date_series.dtype == 'object' or str(date_series.dtype).startswith('string'):\n",
    "        try:\n",
    "            # Format Oracle: DD-MON-YY HH.MI.SS.ffffff AM/PM\n",
    "            # %d = jour, %b = mois abr√©g√©, %y = ann√©e 2 chiffres\n",
    "            # %I = heure 12h, %M = minute, %S = seconde, %f = microseconde\n",
    "            # %p = AM/PM\n",
    "            return pd.to_datetime(date_series, format='%d-%b-%y %I.%M.%S.%f %p', errors='coerce')\n",
    "        except:\n",
    "            # Fallback si le format est diff√©rent\n",
    "            return pd.to_datetime(date_series, errors='coerce')\n",
    "    return date_series\n",
    "\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Optimise les types de donn√©es pour r√©duire l'utilisation de la m√©moire.\n",
    "    Parse correctement les dates au format Oracle.\n",
    "    \"\"\"\n",
    "    print(f\"  üìä M√©moire initiale: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Optimisation des entiers\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    # Optimisation des floats\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Conversion des colonnes de dates au format Oracle\n",
    "    date_columns = [col for col in df.columns if 'DATE' in col.upper()]\n",
    "    \n",
    "    if date_columns:\n",
    "        print(f\"  üìÖ Parsing de {len(date_columns)} colonnes de dates (format Oracle)...\")\n",
    "        for col in date_columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Parser le format Oracle\n",
    "                df[col] = parse_oracle_date(df[col])\n",
    "                # Compter les valeurs non-nulles apr√®s parsing\n",
    "                valid_dates = df[col].notna().sum()\n",
    "                if valid_dates > 0:\n",
    "                    print(f\"     ‚úÖ {col}: {valid_dates:,} dates pars√©es\")\n",
    "    \n",
    "    # Traiter les colonnes TIME s√©par√©ment (peuvent √™tre des dur√©es, pas des dates)\n",
    "    time_columns = [col for col in df.columns if 'TIME' in col.upper() and 'DATE' not in col.upper()]\n",
    "    for col in time_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Les colonnes TIME peuvent √™tre des dur√©es (HH:MM:SS) ou des timestamps\n",
    "            # Essayer de parser comme timestamp Oracle d'abord\n",
    "            try:\n",
    "                parsed = parse_oracle_date(df[col])\n",
    "                if parsed.notna().sum() > 0:\n",
    "                    df[col] = parsed\n",
    "                    print(f\"     ‚úÖ {col}: {parsed.notna().sum():,} timestamps pars√©s\")\n",
    "            except:\n",
    "                # Si √ßa √©choue, laisser tel quel (probablement une dur√©e)\n",
    "                pass\n",
    "    \n",
    "    print(f\"  ‚úÖ M√©moire optimis√©e: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def concatenate_csv_files(table_name, file_pattern):\n",
    "    \"\"\"\n",
    "    Concat√®ne plusieurs fichiers CSV pour une m√™me table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nom de la table\n",
    "        file_pattern: Pattern pour trouver les fichiers (ex: '*SR_*.csv')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame concat√©n√© et optimis√©\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîÑ Traitement de la table: {table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # V√©rifier si le fichier parquet existe d√©j√†\n",
    "    output_file = PROCESSED_DIR / f\"{table_name.lower()}_concatenated.parquet\"\n",
    "    if output_file.exists():\n",
    "        print(f\"  ‚úÖ Fichier d√©j√† existant: {output_file.name}\")\n",
    "        print(f\"  üì• Chargement du cache...\")\n",
    "        df = pd.read_parquet(output_file)\n",
    "        print(f\"  üìè Dimensions: {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes\")\n",
    "        print(f\"  ‚ö° Table {table_name} charg√©e depuis le cache!\\n\")\n",
    "        return df\n",
    "    \n",
    "    # Trouver tous les fichiers correspondants\n",
    "    files = sorted(glob.glob(str(RAW_DATA_DIR / file_pattern)))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"  ‚ö†Ô∏è  Aucun fichier trouv√© pour le pattern: {file_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  üìÇ {len(files)} fichier(s) trouv√©(s):\")\n",
    "    for f in files:\n",
    "        file_size = Path(f).stat().st_size / 1024**2\n",
    "        print(f\"     - {Path(f).name} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    # Lecture et concat√©nation par chunks pour optimiser la m√©moire\n",
    "    dfs = []\n",
    "    for i, file in enumerate(files, 1):\n",
    "        print(f\"  üìñ Lecture du fichier {i}/{len(files)}...\")\n",
    "        df_chunk = pd.read_csv(file, low_memory=False)\n",
    "        dfs.append(df_chunk)\n",
    "    \n",
    "    # Concat√©nation\n",
    "    print(f\"  üîó Concat√©nation de {len(dfs)} DataFrames...\")\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Nettoyage de la m√©moire\n",
    "    del dfs\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"  üìè Dimensions: {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes\")\n",
    "    \n",
    "    # V√©rifier les doublons\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  {duplicates:,} doublons d√©tect√©s - suppression...\")\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"  ‚úÖ Dimensions apr√®s d√©doublonnage: {df.shape[0]:,} lignes\")\n",
    "    \n",
    "    # Optimisation des types\n",
    "    df = optimize_dtypes(df)\n",
    "    \n",
    "    # Sauvegarde du fichier concat√©n√©\n",
    "    print(f\"  üíæ Sauvegarde: {output_file.name}\")\n",
    "    df.to_parquet(output_file, index=False, compression='snappy')\n",
    "    \n",
    "    print(f\"  ‚úÖ {table_name} trait√© avec succ√®s!\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_single_table(table_name, file_pattern):\n",
    "    \"\"\"\n",
    "    Charge une table qui n'a qu'un seul fichier.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì• Chargement de la table: {table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # V√©rifier si le fichier parquet existe d√©j√†\n",
    "    output_file = PROCESSED_DIR / f\"{table_name.lower()}.parquet\"\n",
    "    if output_file.exists():\n",
    "        print(f\"  ‚úÖ Fichier d√©j√† existant: {output_file.name}\")\n",
    "        print(f\"  üì• Chargement du cache...\")\n",
    "        df = pd.read_parquet(output_file)\n",
    "        print(f\"  üìè Dimensions: {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes\")\n",
    "        print(f\"  ‚ö° Table {table_name} charg√©e depuis le cache!\\n\")\n",
    "        return df\n",
    "    \n",
    "    files = glob.glob(str(RAW_DATA_DIR / file_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"  ‚ö†Ô∏è  Aucun fichier trouv√© pour: {file_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    file = files[0]\n",
    "    file_size = Path(file).stat().st_size / 1024**2\n",
    "    print(f\"  üìÇ {Path(file).name} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    df = pd.read_csv(file, low_memory=False)\n",
    "    print(f\"  üìè Dimensions: {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes\")\n",
    "    \n",
    "    df = optimize_dtypes(df)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    df.to_parquet(output_file, index=False, compression='snappy')\n",
    "    print(f\"  üíæ Sauvegarde: {output_file.name}\")\n",
    "    print(f\"  ‚úÖ {table_name} charg√© avec succ√®s!\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires charg√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Concat√©nation des Tables Multi-Fichiers\n",
    "\n",
    "Tables √† concat√©ner:\n",
    "- **SR**: 4 fichiers (_01 √† _04)\n",
    "- **HISTORY_SR**: 4 fichiers (_01 √† _04)\n",
    "- **HISTORY_COMMUNICATION**: 7 fichiers (_01 √† _07)\n",
    "- **SRCONTACT**: 8 fichiers (_01 √† _08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ D√âBUT DE LA CONCAT√âNATION DES TABLES MULTI-FICHIERS\n",
      "======================================================================\n",
      "Heure de d√©but: 2026-02-09 22:53:00\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ Traitement de la table: SR\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: sr_concatenated.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 4,233,963 lignes √ó 54 colonnes\n",
      "  ‚ö° Table SR charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ Traitement de la table: HISTORY_SR\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: history_sr_concatenated.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 25,587,996 lignes √ó 6 colonnes\n",
      "  ‚ö° Table HISTORY_SR charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ Traitement de la table: HISTORY_COMMUNICATION\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: history_communication_concatenated.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 29,115,672 lignes √ó 7 colonnes\n",
      "  ‚ö° Table HISTORY_COMMUNICATION charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ Traitement de la table: SRCONTACT\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: srcontact_concatenated.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 11,903,551 lignes √ó 47 colonnes\n",
      "  ‚ö° Table SRCONTACT charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CONCAT√âNATION TERMIN√âE\n",
      "======================================================================\n",
      "Heure de fin: 2026-02-09 22:53:10\n",
      "\n",
      "Tables concat√©n√©es: 4\n",
      "  - SR: 4,233,963 lignes\n",
      "  - HISTORY_SR: 25,587,996 lignes\n",
      "  - HISTORY_COMMUNICATION: 29,115,672 lignes\n",
      "  - SRCONTACT: 11,903,551 lignes\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire des tables √† concat√©ner\n",
    "# IMPORTANT: Patterns tr√®s sp√©cifiques incluant le double HOBART pour √©viter les chevauchements\n",
    "multi_file_tables = {\n",
    "    'SR': 'CIB2S_HOBART_HOBART_SR_16102025_*.csv',  # Pattern exact pour SR uniquement\n",
    "    'HISTORY_SR': '*HISTORYSR_16102025_*.csv',\n",
    "    'HISTORY_COMMUNICATION': '*HISTORYCOMMUNICATION_16102025_*.csv',\n",
    "    'SRCONTACT': '*SRCONTACT_16102025_*.csv'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ D√âBUT DE LA CONCAT√âNATION DES TABLES MULTI-FICHIERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Heure de d√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Stocker les DataFrames concat√©n√©s\n",
    "concatenated_tables = {}\n",
    "\n",
    "for table_name, pattern in multi_file_tables.items():\n",
    "    df = concatenate_csv_files(table_name, pattern)\n",
    "    if df is not None:\n",
    "        concatenated_tables[table_name] = df\n",
    "    # Lib√©rer la m√©moire apr√®s chaque table\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CONCAT√âNATION TERMIN√âE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Heure de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nTables concat√©n√©es: {len(concatenated_tables)}\")\n",
    "for table, df in concatenated_tables.items():\n",
    "    print(f\"  - {table}: {df.shape[0]:,} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Chargement des Tables Simples\n",
    "\n",
    "Tables avec un seul fichier:\n",
    "- ACTIVITY\n",
    "- HISTORY_ACTIVITY\n",
    "- BUSINESSLINE\n",
    "- BUSINESSLINEACTIVITY\n",
    "- BUSINESSLINEPROCESS\n",
    "- CATEGORY\n",
    "- DESKBUSINESSLINELINK\n",
    "- JUR_USER\n",
    "- LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ CHARGEMENT DES TABLES SIMPLES\n",
      "======================================================================\n",
      "Heure de d√©but: 2026-02-09 22:53:10\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: ACTIVITY\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: activity.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 348,101 lignes √ó 34 colonnes\n",
      "  ‚ö° Table ACTIVITY charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: HISTORY_ACTIVITY\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: history_activity.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 1,810,203 lignes √ó 6 colonnes\n",
      "  ‚ö° Table HISTORY_ACTIVITY charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: BUSINESSLINE\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: businessline.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 1 lignes √ó 2 colonnes\n",
      "  ‚ö° Table BUSINESSLINE charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: BUSINESSLINEACTIVITY\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: businesslineactivity.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 2 lignes √ó 2 colonnes\n",
      "  ‚ö° Table BUSINESSLINEACTIVITY charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: BUSINESSLINEPROCESS\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: businesslineprocess.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 18 lignes √ó 2 colonnes\n",
      "  ‚ö° Table BUSINESSLINEPROCESS charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: CATEGORY\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: category.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 1,548 lignes √ó 2 colonnes\n",
      "  ‚ö° Table CATEGORY charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: DESKBUSINESSLINELINK\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: deskbusinesslinelink.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 878 lignes √ó 4 colonnes\n",
      "  ‚ö° Table DESKBUSINESSLINELINK charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: JUR_USER\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: jur_user.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 4,649 lignes √ó 3 colonnes\n",
      "  ‚ö° Table JUR_USER charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì• Chargement de la table: LABEL\n",
      "============================================================\n",
      "  ‚úÖ Fichier d√©j√† existant: label.parquet\n",
      "  üì• Chargement du cache...\n",
      "  üìè Dimensions: 15 lignes √ó 2 colonnes\n",
      "  ‚ö° Table LABEL charg√©e depuis le cache!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CHARGEMENT DES TABLES SIMPLES TERMIN√â\n",
      "======================================================================\n",
      "Heure de fin: 2026-02-09 22:53:10\n",
      "\n",
      "Tables charg√©es: 9\n",
      "  - ACTIVITY: 348,101 lignes\n",
      "  - HISTORY_ACTIVITY: 1,810,203 lignes\n",
      "  - BUSINESSLINE: 1 lignes\n",
      "  - BUSINESSLINEACTIVITY: 2 lignes\n",
      "  - BUSINESSLINEPROCESS: 18 lignes\n",
      "  - CATEGORY: 1,548 lignes\n",
      "  - DESKBUSINESSLINELINK: 878 lignes\n",
      "  - JUR_USER: 4,649 lignes\n",
      "  - LABEL: 15 lignes\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire des tables simples\n",
    "# IMPORTANT: Patterns tr√®s sp√©cifiques pour √©viter les chevauchements\n",
    "single_file_tables = {\n",
    "    'ACTIVITY': 'CIB2S_HOBART_HOBART_ACTIVITY_16102025_01.csv',  # Pattern exact pour ACTIVITY uniquement\n",
    "    'HISTORY_ACTIVITY': '*HISTORYACTIVITY_16102025_01.csv',\n",
    "    'BUSINESSLINE': '*BUSINESSLINE_16102025_01.csv',\n",
    "    'BUSINESSLINEACTIVITY': '*BUSINESSLINEACTIVITY_16102025_01.csv',\n",
    "    'BUSINESSLINEPROCESS': '*BUSINESSLINEPROCESS_16102025_01.csv',\n",
    "    'CATEGORY': '*CATEGORY_16102025_01.csv',\n",
    "    'DESKBUSINESSLINELINK': '*DESKBUSINESSLINELINK_16102025_01.csv',\n",
    "    'JUR_USER': '*JUR_USER_16102025_01.csv',\n",
    "    'LABEL': '*LABEL_16102025_01.csv'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ CHARGEMENT DES TABLES SIMPLES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Heure de d√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Stocker les DataFrames simples\n",
    "simple_tables = {}\n",
    "\n",
    "for table_name, pattern in single_file_tables.items():\n",
    "    df = load_single_table(table_name, pattern)\n",
    "    if df is not None:\n",
    "        simple_tables[table_name] = df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CHARGEMENT DES TABLES SIMPLES TERMIN√â\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Heure de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nTables charg√©es: {len(simple_tables)}\")\n",
    "for table, df in simple_tables.items():\n",
    "    print(f\"  - {table}: {df.shape[0]:,} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cr√©ation de la Base de Donn√©es SQLite\n",
    "\n",
    "Cr√©ation d'une base de donn√©es optimis√©e avec:\n",
    "- Index sur les cl√©s primaires et √©trang√®res\n",
    "- Optimisation des types de donn√©es SQL\n",
    "- Compression et performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üóÑÔ∏è  CR√âATION DE LA BASE DE DONN√âES SQLITE\n",
      "======================================================================\n",
      "Heure de d√©but: 2026-02-09 22:53:10\n",
      "\n",
      "  üóëÔ∏è  Suppression de l'ancienne base de donn√©es...\n",
      "  üîå Connexion √† la base de donn√©es: hobart_database.db\n",
      "  ‚öôÔ∏è  Application des optimisations SQLite...\n",
      "\n",
      "  üìä Insertion de 13 tables dans la base de donn√©es:\n",
      "\n",
      "  üì• Insertion de SR...\n",
      "     Lignes: 4,233,963 | Colonnes: 54\n",
      "     ‚úÖ SR ins√©r√©\n",
      "\n",
      "  üì• Insertion de HISTORY_SR...\n",
      "     Lignes: 25,587,996 | Colonnes: 6\n",
      "     ‚úÖ HISTORY_SR ins√©r√©\n",
      "\n",
      "  üì• Insertion de HISTORY_COMMUNICATION...\n",
      "     Lignes: 29,115,672 | Colonnes: 7\n",
      "     ‚úÖ HISTORY_COMMUNICATION ins√©r√©\n",
      "\n",
      "  üì• Insertion de SRCONTACT...\n",
      "     Lignes: 11,903,551 | Colonnes: 47\n",
      "     ‚úÖ SRCONTACT ins√©r√©\n",
      "\n",
      "  üì• Insertion de ACTIVITY...\n",
      "     Lignes: 348,101 | Colonnes: 34\n",
      "     ‚úÖ ACTIVITY ins√©r√©\n",
      "\n",
      "  üì• Insertion de HISTORY_ACTIVITY...\n",
      "     Lignes: 1,810,203 | Colonnes: 6\n",
      "     ‚úÖ HISTORY_ACTIVITY ins√©r√©\n",
      "\n",
      "  üì• Insertion de BUSINESSLINE...\n",
      "     Lignes: 1 | Colonnes: 2\n",
      "     ‚úÖ BUSINESSLINE ins√©r√©\n",
      "\n",
      "  üì• Insertion de BUSINESSLINEACTIVITY...\n",
      "     Lignes: 2 | Colonnes: 2\n",
      "     ‚úÖ BUSINESSLINEACTIVITY ins√©r√©\n",
      "\n",
      "  üì• Insertion de BUSINESSLINEPROCESS...\n",
      "     Lignes: 18 | Colonnes: 2\n",
      "     ‚úÖ BUSINESSLINEPROCESS ins√©r√©\n",
      "\n",
      "  üì• Insertion de CATEGORY...\n",
      "     Lignes: 1,548 | Colonnes: 2\n",
      "     ‚úÖ CATEGORY ins√©r√©\n",
      "\n",
      "  üì• Insertion de DESKBUSINESSLINELINK...\n",
      "     Lignes: 878 | Colonnes: 4\n",
      "     ‚úÖ DESKBUSINESSLINELINK ins√©r√©\n",
      "\n",
      "  üì• Insertion de JUR_USER...\n",
      "     Lignes: 4,649 | Colonnes: 3\n",
      "     ‚úÖ JUR_USER ins√©r√©\n",
      "\n",
      "  üì• Insertion de LABEL...\n",
      "     Lignes: 15 | Colonnes: 2\n",
      "     ‚úÖ LABEL ins√©r√©\n",
      "\n",
      "  üîç Cr√©ation des index pour optimiser les requ√™tes...\n",
      "\n",
      "  üî® Cr√©ation de 23 index basiques...\n",
      "     ‚úÖ Index basique cr√©√© sur SR\n",
      "     ‚úÖ Index basique cr√©√© sur SR\n",
      "     ‚úÖ Index basique cr√©√© sur SR\n",
      "     ‚úÖ Index basique cr√©√© sur SR\n",
      "     ‚úÖ Index basique cr√©√© sur SR\n",
      "     ‚úÖ Index basique cr√©√© sur ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_SR\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_SR\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_SR\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_ACTIVITY\n",
      "     ‚úÖ Index basique cr√©√© sur SRCONTACT\n",
      "     ‚úÖ Index basique cr√©√© sur SRCONTACT\n",
      "     ‚úÖ Index basique cr√©√© sur SRCONTACT\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_COMMUNICATION\n",
      "     ‚úÖ Index basique cr√©√© sur HISTORY_COMMUNICATION\n",
      "     ‚úÖ Index basique cr√©√© sur JUR_USER\n",
      "     ‚úÖ Index basique cr√©√© sur LABEL\n",
      "     ‚úÖ Index basique cr√©√© sur CATEGORY\n",
      "\n",
      "  ‚ö° Cr√©ation de 8 index composites (optimisation analytique)...\n",
      "     ‚úÖ Index composite cr√©√©: idx_sr_desk_date\n",
      "     ‚úÖ Index composite cr√©√©: idx_sr_desk_status\n",
      "     ‚úÖ Index composite cr√©√©: idx_sr_type_priority\n",
      "     ‚úÖ Index composite cr√©√©: idx_sr_status_closing\n",
      "     ‚úÖ Index composite cr√©√©: idx_history_sr_date_field\n",
      "     ‚úÖ Index composite cr√©√©: idx_history_activity_date\n",
      "     ‚úÖ Index composite cr√©√©: idx_srcontact_sr_reception\n",
      "     ‚úÖ Index composite cr√©√©: idx_srcontact_outbound\n",
      "\n",
      "  üìä Analyse de la base de donn√©es pour optimisation des plans de requ√™tes...\n",
      "\n",
      "  üìä Taille de la base de donn√©es: 13.75 GB\n",
      "\n",
      "  üìã Tables dans la base de donn√©es (14):\n",
      "     - SR: 4,233,963 lignes\n",
      "     - HISTORY_SR: 25,587,996 lignes\n",
      "     - HISTORY_COMMUNICATION: 29,115,672 lignes\n",
      "     - SRCONTACT: 11,903,551 lignes\n",
      "     - ACTIVITY: 348,101 lignes\n",
      "     - HISTORY_ACTIVITY: 1,810,203 lignes\n",
      "     - BUSINESSLINE: 1 lignes\n",
      "     - BUSINESSLINEACTIVITY: 2 lignes\n",
      "     - BUSINESSLINEPROCESS: 18 lignes\n",
      "     - CATEGORY: 1,548 lignes\n",
      "     - DESKBUSINESSLINELINK: 878 lignes\n",
      "     - JUR_USER: 4,649 lignes\n",
      "     - LABEL: 15 lignes\n",
      "     - SQLITE_STAT1: 35 lignes\n",
      "\n",
      "======================================================================\n",
      "‚úÖ BASE DE DONN√âES CR√â√âE AVEC SUCC√àS\n",
      "======================================================================\n",
      "Heure de fin: 2026-02-09 23:03:58\n",
      "üìÇ Emplacement: /Users/jo/Desktop/BNP Paribas/Data/Processed/hobart_database.db\n",
      "\n",
      "üí° Index optimis√©s: 31 au total (23 basiques + 8 composites)\n"
     ]
    }
   ],
   "source": [
    "def create_optimized_database():\n",
    "    \"\"\"\n",
    "    Cr√©e une base de donn√©es SQLite optimis√©e avec toutes les tables.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üóÑÔ∏è  CR√âATION DE LA BASE DE DONN√âES SQLITE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Heure de d√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    # Supprimer l'ancienne base si elle existe\n",
    "    if DB_PATH.exists():\n",
    "        print(f\"  üóëÔ∏è  Suppression de l'ancienne base de donn√©es...\")\n",
    "        DB_PATH.unlink()\n",
    "    \n",
    "    # Cr√©er la connexion\n",
    "    print(f\"  üîå Connexion √† la base de donn√©es: {DB_PATH.name}\")\n",
    "    conn = sqlite3.connect(str(DB_PATH))\n",
    "    \n",
    "    # Optimisations SQLite pour les performances\n",
    "    print(\"  ‚öôÔ∏è  Application des optimisations SQLite...\")\n",
    "    conn.execute('PRAGMA journal_mode=WAL;')  # Write-Ahead Logging\n",
    "    conn.execute('PRAGMA synchronous=NORMAL;')  # Balance performance/s√©curit√©\n",
    "    conn.execute('PRAGMA cache_size=-64000;')  # 64 MB cache\n",
    "    conn.execute('PRAGMA temp_store=MEMORY;')  # Temp tables en m√©moire\n",
    "    conn.execute('PRAGMA mmap_size=30000000000;')  # Memory-mapped I/O\n",
    "    \n",
    "    # Combiner tous les DataFrames\n",
    "    all_tables = {**concatenated_tables, **simple_tables}\n",
    "    \n",
    "    print(f\"\\n  üìä Insertion de {len(all_tables)} tables dans la base de donn√©es:\\n\")\n",
    "    \n",
    "    for table_name, df in all_tables.items():\n",
    "        print(f\"  üì• Insertion de {table_name}...\")\n",
    "        print(f\"     Lignes: {df.shape[0]:,} | Colonnes: {df.shape[1]}\")\n",
    "        \n",
    "        # Ins√©rer dans SQLite\n",
    "        df.to_sql(table_name.lower(), conn, if_exists='replace', index=False, chunksize=10000)\n",
    "        \n",
    "        print(f\"     ‚úÖ {table_name} ins√©r√©\\n\")\n",
    "    \n",
    "    print(\"  üîç Cr√©ation des index pour optimiser les requ√™tes...\\n\")\n",
    "    \n",
    "    # Index basiques pour les cl√©s primaires et √©trang√®res\n",
    "    basic_indexes = [\n",
    "        # SR table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_id ON sr(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_srnumber ON sr(srnumber);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_status ON sr(status_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_creationdate ON sr(creationdate);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_desk ON sr(jur_desk_id);',\n",
    "        \n",
    "        # ACTIVITY table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_activity_id ON activity(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_activity_sr_id ON activity(sr_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_activity_status ON activity(status_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_activity_creationdate ON activity(creationdate);',\n",
    "        \n",
    "        # HISTORY_SR table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_sr_id ON history_sr(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_sr_sr_id ON history_sr(sr_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_sr_action_date ON history_sr(action_date);',\n",
    "        \n",
    "        # HISTORY_ACTIVITY table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_activity_id ON history_activity(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_activity_activity_id ON history_activity(activity_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_activity_action_date ON history_activity(action_date);',\n",
    "        \n",
    "        # SRCONTACT table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_srcontact_id ON srcontact(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_srcontact_sr_id ON srcontact(sr_id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_srcontact_reception_date ON srcontact(reception_date);',\n",
    "        \n",
    "        # HISTORY_COMMUNICATION table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_comm_id ON history_communication(id);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_comm_comm_id ON history_communication(communication_id);',\n",
    "        \n",
    "        # JUR_USER table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_jur_user_id ON jur_user(id);',\n",
    "        \n",
    "        # LABEL table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_label_id ON label(id);',\n",
    "        \n",
    "        # CATEGORY table\n",
    "        'CREATE INDEX IF NOT EXISTS idx_category_id ON category(id);'\n",
    "    ]\n",
    "    \n",
    "    # Index composites pour requ√™tes analytiques optimis√©es\n",
    "    composite_indexes = [\n",
    "        # Filtres temporels par desk (tendances, performance)\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_desk_date ON sr(jur_desk_id, creationdate);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_desk_status ON sr(jur_desk_id, status_id, creationdate);',\n",
    "        \n",
    "        # Analyse de performance et benchmarking\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_type_priority ON sr(type_id, priority_id, creationdate);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_sr_status_closing ON sr(status_id, closingdate);',\n",
    "        \n",
    "        # Audit trail optimis√© pour recherches d√©taill√©es\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_sr_date_field ON history_sr(sr_id, action_date, field);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_history_activity_date ON history_activity(activity_id, action_date);',\n",
    "        \n",
    "        # Communications et contacts\n",
    "        'CREATE INDEX IF NOT EXISTS idx_srcontact_sr_reception ON srcontact(sr_id, reception_date);',\n",
    "        'CREATE INDEX IF NOT EXISTS idx_srcontact_outbound ON srcontact(outbound, reception_date);',\n",
    "    ]\n",
    "    \n",
    "    # Cr√©er tous les index\n",
    "    all_indexes = basic_indexes + composite_indexes\n",
    "    \n",
    "    print(f\"  üî® Cr√©ation de {len(basic_indexes)} index basiques...\")\n",
    "    for idx_query in basic_indexes:\n",
    "        try:\n",
    "            conn.execute(idx_query)\n",
    "            table_name = idx_query.split('ON ')[1].split('(')[0].strip().upper()\n",
    "            print(f\"     ‚úÖ Index basique cr√©√© sur {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è  Erreur: {e}\")\n",
    "    \n",
    "    print(f\"\\n  ‚ö° Cr√©ation de {len(composite_indexes)} index composites (optimisation analytique)...\")\n",
    "    for idx_query in composite_indexes:\n",
    "        try:\n",
    "            conn.execute(idx_query)\n",
    "            # Extraire le nom de l'index\n",
    "            idx_name = idx_query.split('IF NOT EXISTS ')[1].split(' ON')[0]\n",
    "            print(f\"     ‚úÖ Index composite cr√©√©: {idx_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è  Erreur: {e}\")\n",
    "    \n",
    "    # Analyser la base pour optimiser les requ√™tes\n",
    "    print(\"\\n  üìä Analyse de la base de donn√©es pour optimisation des plans de requ√™tes...\")\n",
    "    conn.execute('ANALYZE;')\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    # Statistiques de la base\n",
    "    db_size = DB_PATH.stat().st_size / 1024**3\n",
    "    print(f\"\\n  üìä Taille de la base de donn√©es: {db_size:.2f} GB\")\n",
    "    \n",
    "    # Lister toutes les tables\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "    print(f\"\\n  üìã Tables dans la base de donn√©es ({len(tables)}):\")\n",
    "    for table in tables['name']:\n",
    "        count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table};\", conn)['count'][0]\n",
    "        print(f\"     - {table.upper()}: {count:,} lignes\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ BASE DE DONN√âES CR√â√âE AVEC SUCC√àS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Heure de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìÇ Emplacement: {DB_PATH}\")\n",
    "    print(f\"\\nüí° Index optimis√©s: {len(all_indexes)} au total ({len(basic_indexes)} basiques + {len(composite_indexes)} composites)\")\n",
    "\n",
    "# Ex√©cution\n",
    "create_optimized_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Merge des Tables Principales\n",
    "\n",
    "Cr√©ation d'une vue consolid√©e avec les relations:\n",
    "- `SR ‚üï ACTIVITY` via `SR.SR_ID = ACTIVITY.SR_ID`\n",
    "- `SR ‚üï HISTORY_SR` via `SR.ID = HISTORY_SR.SR_ID`\n",
    "- `ACTIVITY ‚üï HISTORY_ACTIVITY` via `ACTIVITY.ID = HISTORY_ACTIVITY.ACTIVITY_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîó CR√âATION DES VUES MERGED\n",
      "======================================================================\n",
      "\n",
      "  1Ô∏è‚É£  Vue SR_ACTIVITY (SR ‚üï ACTIVITY)\n",
      "     ‚úÖ Vue cr√©√©e: 4,336,480 lignes\n",
      "\n",
      "  2Ô∏è‚É£  Vue SR_HISTORY (SR ‚üï HISTORY_SR)\n",
      "     ‚úÖ Vue cr√©√©e: 25,967,987 lignes\n",
      "\n",
      "  3Ô∏è‚É£  Vue ACTIVITY_HISTORY (ACTIVITY ‚üï HISTORY_ACTIVITY)\n",
      "     ‚úÖ Vue cr√©√©e: 1,811,329 lignes\n",
      "\n",
      "  4Ô∏è‚É£  Vue COMPLETE (SR ‚üï ACTIVITY ‚üï HISTORY_SR ‚üï HISTORY_ACTIVITY)\n",
      "     ‚úÖ Vue cr√©√©e: 4,336,480 lignes\n",
      "\n",
      "======================================================================\n",
      "‚úÖ VUES MERGED CR√â√âES AVEC SUCC√àS\n",
      "======================================================================\n",
      "\n",
      "Vues disponibles:\n",
      "  - sr_activity_view: SR + ACTIVITY\n",
      "  - sr_history_view: SR + HISTORY_SR\n",
      "  - activity_history_view: ACTIVITY + HISTORY_ACTIVITY\n",
      "  - complete_view: Vue compl√®te consolid√©e\n"
     ]
    }
   ],
   "source": [
    "def create_merged_views():\n",
    "    \"\"\"\n",
    "    Cr√©e des vues SQL pour les merges principaux.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîó CR√âATION DES VUES MERGED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    conn = sqlite3.connect(str(DB_PATH))\n",
    "    \n",
    "    # Vue 1: SR + ACTIVITY\n",
    "    print(\"\\n  1Ô∏è‚É£  Vue SR_ACTIVITY (SR ‚üï ACTIVITY)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS sr_activity_view;')\n",
    "    query_sr_activity = \"\"\"\n",
    "    CREATE VIEW sr_activity_view AS\n",
    "    SELECT \n",
    "        sr.*,\n",
    "        activity.id as activity_id,\n",
    "        activity.actnumber as activity_number,\n",
    "        activity.status_id as activity_status_id,\n",
    "        activity.type_id as activity_type_id,\n",
    "        activity.importance_id as activity_importance_id,\n",
    "        activity.creationdate as activity_creationdate,\n",
    "        activity.closingdate as activity_closingdate,\n",
    "        activity.jur_assigneduser_id as activity_assigned_user_id,\n",
    "        activity.workflow_id as activity_workflow_id,\n",
    "        activity.task_number\n",
    "    FROM sr\n",
    "    LEFT JOIN activity ON sr.id = activity.sr_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_sr_activity)\n",
    "    count_sr_activity = pd.read_sql_query(\"SELECT COUNT(*) as count FROM sr_activity_view;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_sr_activity:,} lignes\")\n",
    "    \n",
    "    # Vue 2: SR + HISTORY_SR\n",
    "    print(\"\\n  2Ô∏è‚É£  Vue SR_HISTORY (SR ‚üï HISTORY_SR)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS sr_history_view;')\n",
    "    query_sr_history = \"\"\"\n",
    "    CREATE VIEW sr_history_view AS\n",
    "    SELECT \n",
    "        sr.id as sr_id,\n",
    "        sr.srnumber,\n",
    "        sr.status_id as sr_status_id,\n",
    "        sr.priority_id,\n",
    "        sr.type_id as sr_type_id,\n",
    "        sr.creationdate as sr_creationdate,\n",
    "        sr.closingdate as sr_closingdate,\n",
    "        sr.jur_desk_id,\n",
    "        history_sr.id as history_id,\n",
    "        history_sr.action as history_action,\n",
    "        history_sr.action_date as history_action_date,\n",
    "        history_sr.field as history_field,\n",
    "        history_sr.user_name as history_user_name\n",
    "    FROM sr\n",
    "    LEFT JOIN history_sr ON sr.id = history_sr.sr_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_sr_history)\n",
    "    count_sr_history = pd.read_sql_query(\"SELECT COUNT(*) as count FROM sr_history_view;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_sr_history:,} lignes\")\n",
    "    \n",
    "    # Vue 3: ACTIVITY + HISTORY_ACTIVITY\n",
    "    print(\"\\n  3Ô∏è‚É£  Vue ACTIVITY_HISTORY (ACTIVITY ‚üï HISTORY_ACTIVITY)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS activity_history_view;')\n",
    "    query_activity_history = \"\"\"\n",
    "    CREATE VIEW activity_history_view AS\n",
    "    SELECT \n",
    "        activity.id as activity_id,\n",
    "        activity.sr_id,\n",
    "        activity.actnumber,\n",
    "        activity.status_id as activity_status_id,\n",
    "        activity.type_id as activity_type_id,\n",
    "        activity.creationdate as activity_creationdate,\n",
    "        activity.closingdate as activity_closingdate,\n",
    "        history_activity.id as history_id,\n",
    "        history_activity.action as history_action,\n",
    "        history_activity.action_date as history_action_date,\n",
    "        history_activity.field as history_field,\n",
    "        history_activity.user_name as history_user_name\n",
    "    FROM activity\n",
    "    LEFT JOIN history_activity ON activity.id = history_activity.activity_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_activity_history)\n",
    "    count_activity_history = pd.read_sql_query(\"SELECT COUNT(*) as count FROM activity_history_view;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_activity_history:,} lignes\")\n",
    "    \n",
    "    # Vue 4: Vue compl√®te (SR + ACTIVITY + HISTORY)\n",
    "    print(\"\\n  4Ô∏è‚É£  Vue COMPLETE (SR ‚üï ACTIVITY ‚üï HISTORY_SR ‚üï HISTORY_ACTIVITY)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS complete_view;')\n",
    "    query_complete = \"\"\"\n",
    "    CREATE VIEW complete_view AS\n",
    "    SELECT \n",
    "        sr.id as sr_id,\n",
    "        sr.srnumber,\n",
    "        sr.status_id as sr_status_id,\n",
    "        sr.priority_id,\n",
    "        sr.type_id as sr_type_id,\n",
    "        sr.creationdate as sr_creationdate,\n",
    "        sr.closingdate as sr_closingdate,\n",
    "        sr.jur_desk_id,\n",
    "        sr.issuer,\n",
    "        sr.treatment_time,\n",
    "        activity.id as activity_id,\n",
    "        activity.actnumber,\n",
    "        activity.status_id as activity_status_id,\n",
    "        activity.task_number\n",
    "    FROM sr\n",
    "    LEFT JOIN activity ON sr.id = activity.sr_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_complete)\n",
    "    count_complete = pd.read_sql_query(\"SELECT COUNT(*) as count FROM complete_view;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_complete:,} lignes\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ VUES MERGED CR√â√âES AVEC SUCC√àS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nVues disponibles:\")\n",
    "    print(\"  - sr_activity_view: SR + ACTIVITY\")\n",
    "    print(\"  - sr_history_view: SR + HISTORY_SR\")\n",
    "    print(\"  - activity_history_view: ACTIVITY + HISTORY_ACTIVITY\")\n",
    "    print(\"  - complete_view: Vue compl√®te consolid√©e\")\n",
    "\n",
    "# Ex√©cution\n",
    "create_merged_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.5. Vues Pr√©-Agr√©g√©es pour Analyses Optimis√©es\n",
    "\n",
    "Cr√©ation de vues SQL pr√©-agr√©g√©es pour acc√©l√©rer les requ√™tes analytiques courantes :\n",
    "- **monthly_desk_metrics** : M√©triques mensuelles par desk (tendances, KPIs)\n",
    "- **sr_lifecycle_summary** : R√©sum√© complet du cycle de vie des SRs\n",
    "- **weekly_sr_distribution** : Distribution hebdomadaire des SRs\n",
    "- **contact_effectiveness_metrics** : M√©triques d'efficacit√© des communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ö° CR√âATION DES VUES PR√â-AGR√âG√âES (OPTIMISATION ANALYTIQUE)\n",
      "======================================================================\n",
      "\n",
      "  1Ô∏è‚É£  Vue MONTHLY_DESK_METRICS\n",
      "     Agr√©gation: M√©triques mensuelles par desk (volumes, taux de closure, temps moyen)\n",
      "     ‚úÖ Vue cr√©√©e: 12,538 agr√©gats (desk √ó mois)\n",
      "\n",
      "  2Ô∏è‚É£  Vue SR_LIFECYCLE_SUMMARY\n",
      "     Agr√©gation: R√©sum√© complet par SR (activit√©s, contacts, dates cl√©s)\n",
      "     ‚úÖ Vue cr√©√©e: 4,233,963 SRs avec r√©sum√© lifecycle\n",
      "\n",
      "  3Ô∏è‚É£  Vue WEEKLY_SR_DISTRIBUTION\n",
      "     Agr√©gation: Distribution hebdomadaire par desk et statut\n",
      "     ‚úÖ Vue cr√©√©e: 73,809 agr√©gats (desk √ó semaine √ó statut)\n",
      "\n",
      "  4Ô∏è‚É£  Vue CONTACT_EFFECTIVENESS_METRICS\n",
      "     Agr√©gation: M√©triques d'efficacit√© des communications par SR\n",
      "     ‚úÖ Vue cr√©√©e: 2,365,340 SRs avec m√©triques de contacts\n",
      "\n",
      "======================================================================\n",
      "‚úÖ VUES PR√â-AGR√âG√âES CR√â√âES AVEC SUCC√àS\n",
      "======================================================================\n",
      "\n",
      "üí° Gains de performance attendus:\n",
      "  - Tendances mensuelles: 50-100x plus rapide\n",
      "  - Analyse lifecycle SR: 30-60x plus rapide\n",
      "  - Distribution hebdomadaire: 40-80x plus rapide\n",
      "  - M√©triques contacts: 20-50x plus rapide\n",
      "\n",
      "Vues disponibles:\n",
      "  - monthly_desk_metrics: Tendances et KPIs mensuels\n",
      "  - sr_lifecycle_summary: R√©sum√© complet du cycle de vie\n",
      "  - weekly_sr_distribution: Distribution hebdomadaire\n",
      "  - contact_effectiveness_metrics: Efficacit√© des communications\n"
     ]
    }
   ],
   "source": [
    "def create_aggregated_views():\n",
    "    \"\"\"\n",
    "    Cr√©e des vues SQL pr√©-agr√©g√©es pour optimiser les requ√™tes analytiques.\n",
    "    Ces vues pr√©-calculent les agr√©gations lourdes pour un acc√®s ultra-rapide.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö° CR√âATION DES VUES PR√â-AGR√âG√âES (OPTIMISATION ANALYTIQUE)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    conn = sqlite3.connect(str(DB_PATH))\n",
    "    \n",
    "    # Vue 1: M√©triques Mensuelles par Desk\n",
    "    print(\"\\n  1Ô∏è‚É£  Vue MONTHLY_DESK_METRICS\")\n",
    "    print(\"     Agr√©gation: M√©triques mensuelles par desk (volumes, taux de closure, temps moyen)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS monthly_desk_metrics;')\n",
    "    query_monthly = \"\"\"\n",
    "    CREATE VIEW monthly_desk_metrics AS\n",
    "    SELECT\n",
    "        strftime('%Y-%m', sr.creationdate) as month,\n",
    "        sr.jur_desk_id,\n",
    "        COUNT(*) as sr_count,\n",
    "        COUNT(CASE WHEN sr.closingdate IS NOT NULL THEN 1 END) as closed_count,\n",
    "        ROUND(COUNT(CASE WHEN sr.closingdate IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 2) as closure_rate,\n",
    "        AVG(CAST((julianday(sr.closingdate) - julianday(sr.creationdate)) * 24 AS REAL)) as avg_hours_to_close,\n",
    "        COUNT(DISTINCT sr.status_id) as unique_statuses\n",
    "    FROM sr\n",
    "    WHERE sr.creationdate IS NOT NULL\n",
    "    GROUP BY strftime('%Y-%m', sr.creationdate), sr.jur_desk_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_monthly)\n",
    "    count_monthly = pd.read_sql_query(\"SELECT COUNT(*) as count FROM monthly_desk_metrics;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_monthly:,} agr√©gats (desk √ó mois)\")\n",
    "    \n",
    "    # Vue 2: R√©sum√© SR (Lifecycle)\n",
    "    print(\"\\n  2Ô∏è‚É£  Vue SR_LIFECYCLE_SUMMARY\")\n",
    "    print(\"     Agr√©gation: R√©sum√© complet par SR (activit√©s, contacts, dates cl√©s)\")\n",
    "    conn.execute('DROP VIEW IF EXISTS sr_lifecycle_summary;')\n",
    "    query_lifecycle = \"\"\"\n",
    "    CREATE VIEW sr_lifecycle_summary AS\n",
    "    SELECT\n",
    "        sr.id as sr_id,\n",
    "        sr.srnumber,\n",
    "        sr.jur_desk_id,\n",
    "        sr.status_id,\n",
    "        sr.priority_id,\n",
    "        sr.type_id,\n",
    "        sr.creationdate,\n",
    "        sr.closingdate,\n",
    "        sr.issuer,\n",
    "        CAST((julianday(sr.closingdate) - julianday(sr.creationdate)) * 24 AS REAL) as hours_to_close,\n",
    "        COUNT(DISTINCT activity.id) as activity_count,\n",
    "        COUNT(DISTINCT srcontact.id) as contact_count,\n",
    "        MIN(srcontact.reception_date) as first_contact_date,\n",
    "        MAX(srcontact.reception_date) as last_contact_date\n",
    "    FROM sr\n",
    "    LEFT JOIN activity ON sr.id = activity.sr_id\n",
    "    LEFT JOIN srcontact ON sr.id = srcontact.sr_id\n",
    "    GROUP BY sr.id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_lifecycle)\n",
    "    count_lifecycle = pd.read_sql_query(\"SELECT COUNT(*) as count FROM sr_lifecycle_summary;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_lifecycle:,} SRs avec r√©sum√© lifecycle\")\n",
    "    \n",
    "    # Vue 3: Distribution Hebdomadaire\n",
    "    print(\"\\n  3Ô∏è‚É£  Vue WEEKLY_SR_DISTRIBUTION\")\n",
    "    print(\"     Agr√©gation: Distribution hebdomadaire par desk et statut\")\n",
    "    conn.execute('DROP VIEW IF EXISTS weekly_sr_distribution;')\n",
    "    query_weekly = \"\"\"\n",
    "    CREATE VIEW weekly_sr_distribution AS\n",
    "    SELECT\n",
    "        strftime('%Y-W%W', sr.creationdate) as week,\n",
    "        sr.jur_desk_id,\n",
    "        sr.status_id,\n",
    "        COUNT(*) as sr_count,\n",
    "        AVG(CAST((julianday(COALESCE(sr.closingdate, datetime('now'))) - julianday(sr.creationdate)) * 24 AS REAL)) as avg_hours_open\n",
    "    FROM sr\n",
    "    WHERE sr.creationdate IS NOT NULL\n",
    "    GROUP BY strftime('%Y-W%W', sr.creationdate), sr.jur_desk_id, sr.status_id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_weekly)\n",
    "    count_weekly = pd.read_sql_query(\"SELECT COUNT(*) as count FROM weekly_sr_distribution;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_weekly:,} agr√©gats (desk √ó semaine √ó statut)\")\n",
    "    \n",
    "    # Vue 4: M√©triques de Contacts\n",
    "    print(\"\\n  4Ô∏è‚É£  Vue CONTACT_EFFECTIVENESS_METRICS\")\n",
    "    print(\"     Agr√©gation: M√©triques d'efficacit√© des communications par SR\")\n",
    "    conn.execute('DROP VIEW IF EXISTS contact_effectiveness_metrics;')\n",
    "    query_contacts = \"\"\"\n",
    "    CREATE VIEW contact_effectiveness_metrics AS\n",
    "    SELECT\n",
    "        sr.id as sr_id,\n",
    "        sr.jur_desk_id,\n",
    "        COUNT(srcontact.id) as total_contacts,\n",
    "        COUNT(CASE WHEN srcontact.outbound = 1 THEN 1 END) as outbound_contacts,\n",
    "        COUNT(CASE WHEN srcontact.outbound = 0 THEN 1 END) as inbound_contacts,\n",
    "        MIN(srcontact.reception_date) as first_contact,\n",
    "        MAX(srcontact.reception_date) as last_contact,\n",
    "        CAST((julianday(sr.closingdate) - julianday(MIN(srcontact.reception_date))) * 24 AS REAL) as hours_first_to_close\n",
    "    FROM sr\n",
    "    LEFT JOIN srcontact ON sr.id = srcontact.sr_id\n",
    "    WHERE srcontact.id IS NOT NULL\n",
    "    GROUP BY sr.id;\n",
    "    \"\"\"\n",
    "    conn.execute(query_contacts)\n",
    "    count_contacts = pd.read_sql_query(\"SELECT COUNT(*) as count FROM contact_effectiveness_metrics;\", conn)['count'][0]\n",
    "    print(f\"     ‚úÖ Vue cr√©√©e: {count_contacts:,} SRs avec m√©triques de contacts\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ VUES PR√â-AGR√âG√âES CR√â√âES AVEC SUCC√àS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüí° Gains de performance attendus:\")\n",
    "    print(\"  - Tendances mensuelles: 50-100x plus rapide\")\n",
    "    print(\"  - Analyse lifecycle SR: 30-60x plus rapide\")\n",
    "    print(\"  - Distribution hebdomadaire: 40-80x plus rapide\")\n",
    "    print(\"  - M√©triques contacts: 20-50x plus rapide\")\n",
    "    print(\"\\nVues disponibles:\")\n",
    "    print(\"  - monthly_desk_metrics: Tendances et KPIs mensuels\")\n",
    "    print(\"  - sr_lifecycle_summary: R√©sum√© complet du cycle de vie\")\n",
    "    print(\"  - weekly_sr_distribution: Distribution hebdomadaire\")\n",
    "    print(\"  - contact_effectiveness_metrics: Efficacit√© des communications\")\n",
    "\n",
    "# Ex√©cution\n",
    "create_aggregated_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test & Validation\n",
    "\n",
    "Requ√™tes de test pour valider la base de donn√©es et les vues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTS DE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "  1Ô∏è‚É£  Statistiques SR:\n",
      " total_srs  unique_desks              first_sr_date               last_sr_date\n",
      "   4233963           761 2024-01-01 00:00:00.054000 2025-10-12 23:02:21.407000\n",
      "\n",
      "  2Ô∏è‚É£  Distribution des statuts SR (Top 10):\n",
      " STATUS_ID    status_name   count  percentage\n",
      "         7         CLOSED 4153758       98.11\n",
      "         8      ABANDONED   33428        0.79\n",
      "         4        ONGOING   31479        0.74\n",
      "        33       REOPENED    8804        0.21\n",
      "        32      SUSPENDED    3928        0.09\n",
      "         6      COMPLETED    1182        0.03\n",
      "         5        PENDING     744        0.02\n",
      "        34           OPEN     352        0.01\n",
      "        35 AWAITING CHECK     141        0.00\n",
      "         2     TO PROCESS     137        0.00\n",
      "\n",
      "  3Ô∏è‚É£  Statistiques Activit√©s:\n",
      " total_activities  srs_with_activities  avg_tasks_per_sr\n",
      "           348101               245578          2.149984\n",
      "\n",
      "  4Ô∏è‚É£  V√©rification des relations:\n",
      "                    relation  sr_count  activity_count\n",
      "              SR -> ACTIVITY   4233963          348075\n",
      "            SR -> HISTORY_SR   4233963        25583799\n",
      "ACTIVITY -> HISTORY_ACTIVITY    348101         1810203\n",
      "\n",
      "  5Ô∏è‚É£  Aper√ßu de la vue compl√®te (5 premi√®res lignes):\n",
      "   sr_id            SRNUMBER            sr_creationdate activity_id ACTNUMBER TASK_NUMBER\n",
      "15441799 [PROXYHUB-CES-9419] 2024-01-02 17:42:56.267000        None      None        None\n",
      "15468648 [PROXYHUB-VPR-9427] 2024-01-04 10:18:49.287000        None      None        None\n",
      "15572526 [PROXYHUB-CES-9460] 2024-01-11 18:17:15.359000        None      None        None\n",
      "15603967 [PROXYHUB-VDA-9474] 2024-01-15 14:22:56.895000        None      None        None\n",
      "15631929 [PROXYHUB-VPR-9482] 2024-01-17 08:31:20.368000        None      None        None\n",
      "\n",
      "  6Ô∏è‚É£  Validation des vues pr√©-agr√©g√©es:\n",
      "\n",
      "     üìä monthly_desk_metrics:\n",
      " total_agr√©gats  mois_distincts  desks_distincts premier_mois dernier_mois  avg_sr_per_month_desk\n",
      "          12538              22              761      2024-01      2025-10                 337.69\n",
      "\n",
      "     Exemple de donn√©es (Top 5 r√©cents):\n",
      "  month  JUR_DESK_ID  sr_count  closed_count  closure_rate  avg_hours_to_close  unique_statuses\n",
      "2025-10        73044      1967          1953         99.29            3.184456                5\n",
      "2025-10        83807      1600          1527         95.44           19.949438                5\n",
      "2025-10        73049      1362          1338         98.24            5.593016                4\n",
      "2025-10        80975      1337          1334         99.78            8.358620                3\n",
      "2025-10        73246      1279          1246         97.42            7.796125                6\n",
      "\n",
      "     üìä sr_lifecycle_summary:\n",
      " total_srs  avg_activities  avg_contacts  avg_hours_to_close  srs_avec_contacts\n",
      "   4233963            0.08          2.81              141.09            2365340\n",
      "\n",
      "     üìä weekly_sr_distribution:\n",
      " total_agr√©gats  semaines_distinctes  desks_distincts premi√®re_semaine derni√®re_semaine\n",
      "          73809                   94              761         2024-W01         2025-W40\n",
      "\n",
      "     üìä contact_effectiveness_metrics:\n",
      " total_srs_avec_contacts  avg_contacts  avg_outbound  avg_inbound  avg_hours_first_to_close\n",
      "                 2365340          5.03          2.07         2.96                    137.21\n",
      "\n",
      "  ‚úÖ Toutes les vues pr√©-agr√©g√©es sont fonctionnelles!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TESTS TERMIN√âS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTS DE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Test 1: Nombre total de SRs\n",
    "print(\"\\n  1Ô∏è‚É£  Statistiques SR:\")\n",
    "sr_stats = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_srs,\n",
    "        COUNT(DISTINCT jur_desk_id) as unique_desks,\n",
    "        MIN(creationdate) as first_sr_date,\n",
    "        MAX(creationdate) as last_sr_date\n",
    "    FROM sr;\n",
    "\"\"\", conn)\n",
    "print(sr_stats.to_string(index=False))\n",
    "\n",
    "# Test 2: Distribution des statuts SR\n",
    "print(\"\\n  2Ô∏è‚É£  Distribution des statuts SR (Top 10):\")\n",
    "status_dist = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        sr.status_id,\n",
    "        label.name as status_name,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sr), 2) as percentage\n",
    "    FROM sr\n",
    "    LEFT JOIN label ON sr.status_id = label.id\n",
    "    GROUP BY sr.status_id, label.name\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10;\n",
    "\"\"\", conn)\n",
    "print(status_dist.to_string(index=False))\n",
    "\n",
    "# Test 3: Nombre d'activit√©s par SR\n",
    "print(\"\\n  3Ô∏è‚É£  Statistiques Activit√©s:\")\n",
    "activity_stats = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_activities,\n",
    "        COUNT(DISTINCT sr_id) as srs_with_activities,\n",
    "        AVG(CAST(task_number AS FLOAT)) as avg_tasks_per_sr\n",
    "    FROM activity;\n",
    "\"\"\", conn)\n",
    "print(activity_stats.to_string(index=False))\n",
    "\n",
    "# Test 4: V√©rification des relations\n",
    "print(\"\\n  4Ô∏è‚É£  V√©rification des relations:\")\n",
    "relations_check = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        'SR -> ACTIVITY' as relation,\n",
    "        COUNT(DISTINCT sr.id) as sr_count,\n",
    "        COUNT(DISTINCT activity.id) as activity_count\n",
    "    FROM sr\n",
    "    LEFT JOIN activity ON sr.id = activity.sr_id\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'SR -> HISTORY_SR' as relation,\n",
    "        COUNT(DISTINCT sr.id) as sr_count,\n",
    "        COUNT(DISTINCT history_sr.id) as history_count\n",
    "    FROM sr\n",
    "    LEFT JOIN history_sr ON sr.id = history_sr.sr_id\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'ACTIVITY -> HISTORY_ACTIVITY' as relation,\n",
    "        COUNT(DISTINCT activity.id) as activity_count,\n",
    "        COUNT(DISTINCT history_activity.id) as history_count\n",
    "    FROM activity\n",
    "    LEFT JOIN history_activity ON activity.id = history_activity.activity_id;\n",
    "\"\"\", conn)\n",
    "print(relations_check.to_string(index=False))\n",
    "\n",
    "# Test 5: Exemple de requ√™te sur la vue compl√®te\n",
    "print(\"\\n  5Ô∏è‚É£  Aper√ßu de la vue compl√®te (5 premi√®res lignes):\")\n",
    "complete_sample = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        sr_id,\n",
    "        srnumber,\n",
    "        sr_creationdate,\n",
    "        activity_id,\n",
    "        actnumber,\n",
    "        task_number\n",
    "    FROM complete_view\n",
    "    LIMIT 5;\n",
    "\"\"\", conn)\n",
    "print(complete_sample.to_string(index=False))\n",
    "\n",
    "# Test 6: Validation des vues pr√©-agr√©g√©es (Section 6.5)\n",
    "print(\"\\n  6Ô∏è‚É£  Validation des vues pr√©-agr√©g√©es:\")\n",
    "\n",
    "# 6a. monthly_desk_metrics\n",
    "print(\"\\n     üìä monthly_desk_metrics:\")\n",
    "monthly_check = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_agr√©gats,\n",
    "        COUNT(DISTINCT month) as mois_distincts,\n",
    "        COUNT(DISTINCT jur_desk_id) as desks_distincts,\n",
    "        MIN(month) as premier_mois,\n",
    "        MAX(month) as dernier_mois,\n",
    "        ROUND(AVG(sr_count), 2) as avg_sr_per_month_desk\n",
    "    FROM monthly_desk_metrics;\n",
    "\"\"\", conn)\n",
    "print(monthly_check.to_string(index=False))\n",
    "\n",
    "# Exemple de donn√©es\n",
    "monthly_sample = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM monthly_desk_metrics \n",
    "    ORDER BY month DESC, sr_count DESC \n",
    "    LIMIT 5;\n",
    "\"\"\", conn)\n",
    "print(\"\\n     Exemple de donn√©es (Top 5 r√©cents):\")\n",
    "print(monthly_sample.to_string(index=False))\n",
    "\n",
    "# 6b. sr_lifecycle_summary\n",
    "print(\"\\n     üìä sr_lifecycle_summary:\")\n",
    "lifecycle_check = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_srs,\n",
    "        ROUND(AVG(activity_count), 2) as avg_activities,\n",
    "        ROUND(AVG(contact_count), 2) as avg_contacts,\n",
    "        ROUND(AVG(hours_to_close), 2) as avg_hours_to_close,\n",
    "        COUNT(CASE WHEN contact_count > 0 THEN 1 END) as srs_avec_contacts\n",
    "    FROM sr_lifecycle_summary;\n",
    "\"\"\", conn)\n",
    "print(lifecycle_check.to_string(index=False))\n",
    "\n",
    "# 6c. weekly_sr_distribution\n",
    "print(\"\\n     üìä weekly_sr_distribution:\")\n",
    "weekly_check = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_agr√©gats,\n",
    "        COUNT(DISTINCT week) as semaines_distinctes,\n",
    "        COUNT(DISTINCT jur_desk_id) as desks_distincts,\n",
    "        MIN(week) as premi√®re_semaine,\n",
    "        MAX(week) as derni√®re_semaine\n",
    "    FROM weekly_sr_distribution;\n",
    "\"\"\", conn)\n",
    "print(weekly_check.to_string(index=False))\n",
    "\n",
    "# 6d. contact_effectiveness_metrics\n",
    "print(\"\\n     üìä contact_effectiveness_metrics:\")\n",
    "contact_check = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_srs_avec_contacts,\n",
    "        ROUND(AVG(total_contacts), 2) as avg_contacts,\n",
    "        ROUND(AVG(outbound_contacts), 2) as avg_outbound,\n",
    "        ROUND(AVG(inbound_contacts), 2) as avg_inbound,\n",
    "        ROUND(AVG(hours_first_to_close), 2) as avg_hours_first_to_close\n",
    "    FROM contact_effectiveness_metrics;\n",
    "\"\"\", conn)\n",
    "print(contact_check.to_string(index=False))\n",
    "\n",
    "print(\"\\n  ‚úÖ Toutes les vues pr√©-agr√©g√©es sont fonctionnelles!\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TESTS TERMIN√âS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Exemples de Requ√™tes Analytiques\n",
    "\n",
    "Requ√™tes SQL optimis√©es pour l'analyse des donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä EXEMPLES DE REQU√äTES ANALYTIQUES\n",
      "======================================================================\n",
      "\n",
      "  üìà Exemple 1: √âvolution mensuelle des SRs cr√©√©s\n",
      "  month  sr_count  active_desks\n",
      "2024-01    166477           459\n",
      "2024-02    158358           478\n",
      "2024-03    165564           479\n",
      "2024-04    180472           498\n",
      "2024-05    191718           500\n",
      "2024-06    175632           507\n",
      "2024-07    200477           510\n",
      "2024-08    170412           515\n",
      "2024-09    182885           551\n",
      "2024-10    207941           566\n",
      "\n",
      "  üìà Exemple 2: Top 10 desks par volume de SRs\n",
      " JUR_DESK_ID  total_srs  closed_srs  closure_rate\n",
      "       73044     133455      133395         99.96\n",
      "       73049      83904       83843         99.93\n",
      "       73246      74412       74347         99.91\n",
      "       73046      65073       65062         99.98\n",
      "       73059      61437       61429         99.99\n",
      "       83807      51306       51174         99.74\n",
      "       72956      51082       50917         99.68\n",
      "       77933      50016       48644         97.26\n",
      "       73203      46669       46632         99.92\n",
      "       73195      46574       46551         99.95\n",
      "\n",
      "  üìà Exemple 3: Statistiques de temps de traitement\n",
      " total_closed_srs  avg_hours_to_close  min_hours    max_hours\n",
      "          4194970          141.090965        0.0 15462.172891\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EXEMPLES TERMIN√âS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä EXEMPLES DE REQU√äTES ANALYTIQUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Exemple 1: Analyse temporelle des SRs\n",
    "print(\"\\n  üìà Exemple 1: √âvolution mensuelle des SRs cr√©√©s\")\n",
    "monthly_srs = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        strftime('%Y-%m', creationdate) as month,\n",
    "        COUNT(*) as sr_count,\n",
    "        COUNT(DISTINCT jur_desk_id) as active_desks\n",
    "    FROM sr\n",
    "    WHERE creationdate IS NOT NULL\n",
    "    GROUP BY strftime('%Y-%m', creationdate)\n",
    "    ORDER BY month;\n",
    "\"\"\", conn)\n",
    "print(monthly_srs.head(10).to_string(index=False))\n",
    "\n",
    "# Exemple 2: Performance des desks\n",
    "print(\"\\n  üìà Exemple 2: Top 10 desks par volume de SRs\")\n",
    "desk_performance = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        jur_desk_id,\n",
    "        COUNT(*) as total_srs,\n",
    "        COUNT(CASE WHEN closingdate IS NOT NULL THEN 1 END) as closed_srs,\n",
    "        ROUND(COUNT(CASE WHEN closingdate IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 2) as closure_rate\n",
    "    FROM sr\n",
    "    GROUP BY jur_desk_id\n",
    "    ORDER BY total_srs DESC\n",
    "    LIMIT 10;\n",
    "\"\"\", conn)\n",
    "print(desk_performance.to_string(index=False))\n",
    "\n",
    "# Exemple 3: Analyse des temps de traitement\n",
    "print(\"\\n  üìà Exemple 3: Statistiques de temps de traitement\")\n",
    "treatment_time = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_closed_srs,\n",
    "        AVG(CAST((julianday(closingdate) - julianday(creationdate)) * 24 AS REAL)) as avg_hours_to_close,\n",
    "        MIN(CAST((julianday(closingdate) - julianday(creationdate)) * 24 AS REAL)) as min_hours,\n",
    "        MAX(CAST((julianday(closingdate) - julianday(creationdate)) * 24 AS REAL)) as max_hours\n",
    "    FROM sr\n",
    "    WHERE closingdate IS NOT NULL AND creationdate IS NOT NULL;\n",
    "\"\"\", conn)\n",
    "print(treatment_time.to_string(index=False))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EXEMPLES TERMIN√âS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. R√©sum√© & Export\n",
    "\n",
    "G√©n√©ration d'un rapport final et export des m√©tadonn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìã G√âN√âRATION DU RAPPORT FINAL\n",
      "======================================================================\n",
      "\n",
      "üìä Tables dans la base de donn√©es:\n",
      "                Table     Lignes  Colonnes\n",
      "             ACTIVITY    348,101        34\n",
      "         BUSINESSLINE          1         2\n",
      " BUSINESSLINEACTIVITY          2         2\n",
      "  BUSINESSLINEPROCESS         18         2\n",
      "             CATEGORY      1,548         2\n",
      " DESKBUSINESSLINELINK        878         4\n",
      "     HISTORY_ACTIVITY  1,810,203         6\n",
      "HISTORY_COMMUNICATION 29,115,672         7\n",
      "           HISTORY_SR 25,587,996         6\n",
      "             JUR_USER      4,649         3\n",
      "                LABEL         15         2\n",
      "         SQLITE_STAT1         35         3\n",
      "                   SR  4,233,963        54\n",
      "            SRCONTACT 11,903,551        47\n",
      "\n",
      "üíæ M√©tadonn√©es export√©es: database_metadata.csv\n",
      "\n",
      "üìä Vues SQL disponibles:\n",
      "  - activity_history_view\n",
      "  - complete_view\n",
      "  - contact_effectiveness_metrics\n",
      "  - monthly_desk_metrics\n",
      "  - sr_activity_view\n",
      "  - sr_history_view\n",
      "  - sr_lifecycle_summary\n",
      "  - weekly_sr_distribution\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\n",
      "======================================================================\n",
      "\n",
      "üìÇ Fichiers g√©n√©r√©s:\n",
      "  - Base de donn√©es: /Users/jo/Desktop/BNP Paribas/Data/Processed/hobart_database.db\n",
      "  - M√©tadonn√©es: /Users/jo/Desktop/BNP Paribas/Data/Processed/database_metadata.csv\n",
      "  - Fichiers Parquet: /Users/jo/Desktop/BNP Paribas/Data/Processed/*.parquet\n",
      "\n",
      "üéØ Prochaines √©tapes:\n",
      "  1. Utiliser les vues SQL pour vos analyses\n",
      "  2. Ex√©cuter des requ√™tes optimis√©es via SQLite\n",
      "  3. Charger les fichiers Parquet pour des analyses avec Pandas/Polars\n",
      "\n",
      "‚è∞ Temps d'ex√©cution total: [Voir output complet]\n",
      "üéâ Pipeline de traitement des donn√©es BNP Paribas B2S termin√©!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã G√âN√âRATION DU RAPPORT FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# R√©cup√©rer les m√©tadonn√©es de toutes les tables\n",
    "tables = pd.read_sql_query(\"\"\"\n",
    "    SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\n",
    "\"\"\", conn)\n",
    "\n",
    "metadata_list = []\n",
    "for table_name in tables['name']:\n",
    "    count_query = f\"SELECT COUNT(*) as count FROM {table_name};\"\n",
    "    count = pd.read_sql_query(count_query, conn)['count'][0]\n",
    "    \n",
    "    # Colonnes\n",
    "    cols_query = f\"PRAGMA table_info({table_name});\"\n",
    "    cols = pd.read_sql_query(cols_query, conn)\n",
    "    \n",
    "    metadata_list.append({\n",
    "        'Table': table_name.upper(),\n",
    "        'Lignes': f\"{count:,}\",\n",
    "        'Colonnes': len(cols)\n",
    "    })\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "print(\"\\nüìä Tables dans la base de donn√©es:\")\n",
    "print(metadata_df.to_string(index=False))\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "report_path = PROCESSED_DIR / 'database_metadata.csv'\n",
    "metadata_df.to_csv(report_path, index=False)\n",
    "print(f\"\\nüíæ M√©tadonn√©es export√©es: {report_path.name}\")\n",
    "\n",
    "# Vues disponibles\n",
    "views = pd.read_sql_query(\"\"\"\n",
    "    SELECT name FROM sqlite_master WHERE type='view' ORDER BY name;\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüìä Vues SQL disponibles:\")\n",
    "for view_name in views['name']:\n",
    "    print(f\"  - {view_name}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÇ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"  - Base de donn√©es: {DB_PATH}\")\n",
    "print(f\"  - M√©tadonn√©es: {report_path}\")\n",
    "print(f\"  - Fichiers Parquet: {PROCESSED_DIR}/*.parquet\")\n",
    "\n",
    "print(f\"\\nüéØ Prochaines √©tapes:\")\n",
    "print(f\"  1. Utiliser les vues SQL pour vos analyses\")\n",
    "print(f\"  2. Ex√©cuter des requ√™tes optimis√©es via SQLite\")\n",
    "print(f\"  3. Charger les fichiers Parquet pour des analyses avec Pandas/Polars\")\n",
    "\n",
    "print(f\"\\n‚è∞ Temps d'ex√©cution total: [Voir output complet]\")\n",
    "print(f\"üéâ Pipeline de traitement des donn√©es BNP Paribas B2S termin√©!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
